{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./emotion_train.csv')\n",
    "val_data = pd.read_csv('./emotion_validation.csv')\n",
    "test_data = pd.read_csv('./emotion_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_labels = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "train_texts = train_data['text'].tolist()\n",
    "train_labels = train_data['label'].tolist()\n",
    "val_texts = val_data['text'].tolist()\n",
    "val_labels = val_data['label'].tolist()\n",
    "test_texts = test_data['text'].tolist()\n",
    "test_labels = test_data['label'].tolist()\n",
    "\n",
    "train_dataset = EmotionDataset(train_texts, train_labels, tokenizer, max_length=128)\n",
    "val_dataset = EmotionDataset(val_texts, val_labels, tokenizer, max_length=128)\n",
    "test_dataset = EmotionDataset(test_texts, test_labels, tokenizer, max_length=128)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"f1\": f1_score(labels, predictions, average=\"weighted\"),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=len(emotion_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_24092\\2084584541.py:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d6496cbccf4e10b41e470bc7bea433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.736, 'grad_norm': 3.422088861465454, 'learning_rate': 1.9933333333333334e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6838, 'grad_norm': 6.059967994689941, 'learning_rate': 1.9866666666666667e-05, 'epoch': 0.02}\n",
      "{'loss': 1.7347, 'grad_norm': 9.450512886047363, 'learning_rate': 1.98e-05, 'epoch': 0.03}\n",
      "{'loss': 1.6665, 'grad_norm': 4.085999488830566, 'learning_rate': 1.9733333333333336e-05, 'epoch': 0.04}\n",
      "{'loss': 1.6177, 'grad_norm': 6.0765581130981445, 'learning_rate': 1.9666666666666666e-05, 'epoch': 0.05}\n",
      "{'loss': 1.571, 'grad_norm': 6.820758819580078, 'learning_rate': 1.9600000000000002e-05, 'epoch': 0.06}\n",
      "{'loss': 1.4835, 'grad_norm': 7.022929668426514, 'learning_rate': 1.9533333333333335e-05, 'epoch': 0.07}\n",
      "{'loss': 1.455, 'grad_norm': 26.703659057617188, 'learning_rate': 1.9466666666666668e-05, 'epoch': 0.08}\n",
      "{'loss': 1.3325, 'grad_norm': 68.47493743896484, 'learning_rate': 1.94e-05, 'epoch': 0.09}\n",
      "{'loss': 1.3637, 'grad_norm': 16.150217056274414, 'learning_rate': 1.9333333333333333e-05, 'epoch': 0.1}\n",
      "{'loss': 1.209, 'grad_norm': 13.321219444274902, 'learning_rate': 1.926666666666667e-05, 'epoch': 0.11}\n",
      "{'loss': 1.2288, 'grad_norm': 24.102710723876953, 'learning_rate': 1.9200000000000003e-05, 'epoch': 0.12}\n",
      "{'loss': 1.2977, 'grad_norm': 14.554994583129883, 'learning_rate': 1.9133333333333335e-05, 'epoch': 0.13}\n",
      "{'loss': 1.2949, 'grad_norm': 9.197545051574707, 'learning_rate': 1.9066666666666668e-05, 'epoch': 0.14}\n",
      "{'loss': 1.3034, 'grad_norm': 66.82637786865234, 'learning_rate': 1.9e-05, 'epoch': 0.15}\n",
      "{'loss': 1.1925, 'grad_norm': 7.566648006439209, 'learning_rate': 1.8933333333333334e-05, 'epoch': 0.16}\n",
      "{'loss': 1.1133, 'grad_norm': 10.759594917297363, 'learning_rate': 1.886666666666667e-05, 'epoch': 0.17}\n",
      "{'loss': 1.109, 'grad_norm': 27.102745056152344, 'learning_rate': 1.88e-05, 'epoch': 0.18}\n",
      "{'loss': 0.9225, 'grad_norm': 6.089676380157471, 'learning_rate': 1.8733333333333336e-05, 'epoch': 0.19}\n",
      "{'loss': 1.0876, 'grad_norm': 30.99205780029297, 'learning_rate': 1.866666666666667e-05, 'epoch': 0.2}\n",
      "{'loss': 1.0881, 'grad_norm': 16.512832641601562, 'learning_rate': 1.86e-05, 'epoch': 0.21}\n",
      "{'loss': 1.0815, 'grad_norm': 24.457014083862305, 'learning_rate': 1.8533333333333334e-05, 'epoch': 0.22}\n",
      "{'loss': 1.0071, 'grad_norm': 49.85853958129883, 'learning_rate': 1.8466666666666667e-05, 'epoch': 0.23}\n",
      "{'loss': 1.049, 'grad_norm': 15.879866600036621, 'learning_rate': 1.8400000000000003e-05, 'epoch': 0.24}\n",
      "{'loss': 0.9019, 'grad_norm': 54.82399368286133, 'learning_rate': 1.8333333333333333e-05, 'epoch': 0.25}\n",
      "{'loss': 0.9351, 'grad_norm': 25.35643768310547, 'learning_rate': 1.826666666666667e-05, 'epoch': 0.26}\n",
      "{'loss': 0.9337, 'grad_norm': 42.793739318847656, 'learning_rate': 1.8200000000000002e-05, 'epoch': 0.27}\n",
      "{'loss': 0.8374, 'grad_norm': 15.497185707092285, 'learning_rate': 1.8133333333333335e-05, 'epoch': 0.28}\n",
      "{'loss': 0.8105, 'grad_norm': 33.27052688598633, 'learning_rate': 1.8066666666666668e-05, 'epoch': 0.29}\n",
      "{'loss': 0.7462, 'grad_norm': 15.56624984741211, 'learning_rate': 1.8e-05, 'epoch': 0.3}\n",
      "{'loss': 0.8388, 'grad_norm': 20.597707748413086, 'learning_rate': 1.7933333333333333e-05, 'epoch': 0.31}\n",
      "{'loss': 0.8516, 'grad_norm': 19.735363006591797, 'learning_rate': 1.7866666666666666e-05, 'epoch': 0.32}\n",
      "{'loss': 0.6936, 'grad_norm': 18.645280838012695, 'learning_rate': 1.7800000000000002e-05, 'epoch': 0.33}\n",
      "{'loss': 0.6937, 'grad_norm': 11.479896545410156, 'learning_rate': 1.7733333333333335e-05, 'epoch': 0.34}\n",
      "{'loss': 0.6992, 'grad_norm': 12.223499298095703, 'learning_rate': 1.7666666666666668e-05, 'epoch': 0.35}\n",
      "{'loss': 0.7042, 'grad_norm': 36.949798583984375, 'learning_rate': 1.76e-05, 'epoch': 0.36}\n",
      "{'loss': 0.6648, 'grad_norm': 11.664534568786621, 'learning_rate': 1.7533333333333337e-05, 'epoch': 0.37}\n",
      "{'loss': 0.6358, 'grad_norm': 14.732762336730957, 'learning_rate': 1.7466666666666667e-05, 'epoch': 0.38}\n",
      "{'loss': 0.4569, 'grad_norm': 5.7594451904296875, 'learning_rate': 1.7400000000000003e-05, 'epoch': 0.39}\n",
      "{'loss': 0.5772, 'grad_norm': 28.26827621459961, 'learning_rate': 1.7333333333333336e-05, 'epoch': 0.4}\n",
      "{'loss': 0.6002, 'grad_norm': 13.006314277648926, 'learning_rate': 1.726666666666667e-05, 'epoch': 0.41}\n",
      "{'loss': 0.524, 'grad_norm': 19.758392333984375, 'learning_rate': 1.72e-05, 'epoch': 0.42}\n",
      "{'loss': 0.5413, 'grad_norm': 22.461668014526367, 'learning_rate': 1.7133333333333334e-05, 'epoch': 0.43}\n",
      "{'loss': 0.4942, 'grad_norm': 27.70783233642578, 'learning_rate': 1.706666666666667e-05, 'epoch': 0.44}\n",
      "{'loss': 0.4247, 'grad_norm': 57.11034393310547, 'learning_rate': 1.7e-05, 'epoch': 0.45}\n",
      "{'loss': 0.573, 'grad_norm': 10.061210632324219, 'learning_rate': 1.6933333333333336e-05, 'epoch': 0.46}\n",
      "{'loss': 0.378, 'grad_norm': 12.598586082458496, 'learning_rate': 1.686666666666667e-05, 'epoch': 0.47}\n",
      "{'loss': 0.3873, 'grad_norm': 18.206480026245117, 'learning_rate': 1.6800000000000002e-05, 'epoch': 0.48}\n",
      "{'loss': 0.4538, 'grad_norm': 9.998026847839355, 'learning_rate': 1.6733333333333335e-05, 'epoch': 0.49}\n",
      "{'loss': 0.3281, 'grad_norm': 11.09135913848877, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.5}\n",
      "{'loss': 0.4459, 'grad_norm': 10.966815948486328, 'learning_rate': 1.66e-05, 'epoch': 0.51}\n",
      "{'loss': 0.4857, 'grad_norm': 28.969608306884766, 'learning_rate': 1.6533333333333333e-05, 'epoch': 0.52}\n",
      "{'loss': 0.4743, 'grad_norm': 9.802875518798828, 'learning_rate': 1.646666666666667e-05, 'epoch': 0.53}\n",
      "{'loss': 0.4903, 'grad_norm': 46.3074951171875, 'learning_rate': 1.64e-05, 'epoch': 0.54}\n",
      "{'loss': 0.3463, 'grad_norm': 6.095859050750732, 'learning_rate': 1.6333333333333335e-05, 'epoch': 0.55}\n",
      "{'loss': 0.5082, 'grad_norm': 29.944089889526367, 'learning_rate': 1.6266666666666668e-05, 'epoch': 0.56}\n",
      "{'loss': 0.3297, 'grad_norm': 20.70134162902832, 'learning_rate': 1.62e-05, 'epoch': 0.57}\n",
      "{'loss': 0.393, 'grad_norm': 5.917272090911865, 'learning_rate': 1.6133333333333334e-05, 'epoch': 0.58}\n",
      "{'loss': 0.3596, 'grad_norm': 21.440105438232422, 'learning_rate': 1.606666666666667e-05, 'epoch': 0.59}\n",
      "{'loss': 0.4115, 'grad_norm': 16.69637107849121, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.6}\n",
      "{'loss': 0.4566, 'grad_norm': 15.496953964233398, 'learning_rate': 1.5933333333333336e-05, 'epoch': 0.61}\n",
      "{'loss': 0.305, 'grad_norm': 19.055747985839844, 'learning_rate': 1.586666666666667e-05, 'epoch': 0.62}\n",
      "{'loss': 0.4798, 'grad_norm': 73.09230041503906, 'learning_rate': 1.58e-05, 'epoch': 0.63}\n",
      "{'loss': 0.5142, 'grad_norm': 21.645689010620117, 'learning_rate': 1.5733333333333334e-05, 'epoch': 0.64}\n",
      "{'loss': 0.3829, 'grad_norm': 17.207645416259766, 'learning_rate': 1.5666666666666667e-05, 'epoch': 0.65}\n",
      "{'loss': 0.4121, 'grad_norm': 21.723718643188477, 'learning_rate': 1.5600000000000003e-05, 'epoch': 0.66}\n",
      "{'loss': 0.2567, 'grad_norm': 8.219390869140625, 'learning_rate': 1.5533333333333333e-05, 'epoch': 0.67}\n",
      "{'loss': 0.3751, 'grad_norm': 30.2606143951416, 'learning_rate': 1.546666666666667e-05, 'epoch': 0.68}\n",
      "{'loss': 0.3239, 'grad_norm': 11.827608108520508, 'learning_rate': 1.54e-05, 'epoch': 0.69}\n",
      "{'loss': 0.3401, 'grad_norm': 13.32103157043457, 'learning_rate': 1.5333333333333334e-05, 'epoch': 0.7}\n",
      "{'loss': 0.3197, 'grad_norm': 12.575152397155762, 'learning_rate': 1.5266666666666667e-05, 'epoch': 0.71}\n",
      "{'loss': 0.4115, 'grad_norm': 19.033754348754883, 'learning_rate': 1.5200000000000002e-05, 'epoch': 0.72}\n",
      "{'loss': 0.2721, 'grad_norm': 78.53526306152344, 'learning_rate': 1.5133333333333335e-05, 'epoch': 0.73}\n",
      "{'loss': 0.3814, 'grad_norm': 44.68888473510742, 'learning_rate': 1.5066666666666668e-05, 'epoch': 0.74}\n",
      "{'loss': 0.4165, 'grad_norm': 39.91813659667969, 'learning_rate': 1.5000000000000002e-05, 'epoch': 0.75}\n",
      "{'loss': 0.2653, 'grad_norm': 7.1972856521606445, 'learning_rate': 1.4933333333333335e-05, 'epoch': 0.76}\n",
      "{'loss': 0.3666, 'grad_norm': 23.194374084472656, 'learning_rate': 1.4866666666666668e-05, 'epoch': 0.77}\n",
      "{'loss': 0.3163, 'grad_norm': 11.342711448669434, 'learning_rate': 1.48e-05, 'epoch': 0.78}\n",
      "{'loss': 0.4355, 'grad_norm': 21.29683494567871, 'learning_rate': 1.4733333333333335e-05, 'epoch': 0.79}\n",
      "{'loss': 0.3878, 'grad_norm': 27.77368927001953, 'learning_rate': 1.4666666666666666e-05, 'epoch': 0.8}\n",
      "{'loss': 0.4531, 'grad_norm': 28.972673416137695, 'learning_rate': 1.46e-05, 'epoch': 0.81}\n",
      "{'loss': 0.2745, 'grad_norm': 17.313783645629883, 'learning_rate': 1.4533333333333335e-05, 'epoch': 0.82}\n",
      "{'loss': 0.2795, 'grad_norm': 19.906036376953125, 'learning_rate': 1.4466666666666668e-05, 'epoch': 0.83}\n",
      "{'loss': 0.2901, 'grad_norm': 30.10358428955078, 'learning_rate': 1.4400000000000001e-05, 'epoch': 0.84}\n",
      "{'loss': 0.3516, 'grad_norm': 11.460126876831055, 'learning_rate': 1.4333333333333334e-05, 'epoch': 0.85}\n",
      "{'loss': 0.3701, 'grad_norm': 13.458196640014648, 'learning_rate': 1.4266666666666668e-05, 'epoch': 0.86}\n",
      "{'loss': 0.2207, 'grad_norm': 25.811302185058594, 'learning_rate': 1.4200000000000001e-05, 'epoch': 0.87}\n",
      "{'loss': 0.3059, 'grad_norm': 54.66029357910156, 'learning_rate': 1.4133333333333334e-05, 'epoch': 0.88}\n",
      "{'loss': 0.2151, 'grad_norm': 2.4061150550842285, 'learning_rate': 1.4066666666666669e-05, 'epoch': 0.89}\n",
      "{'loss': 0.3045, 'grad_norm': 26.110994338989258, 'learning_rate': 1.4e-05, 'epoch': 0.9}\n",
      "{'loss': 0.3214, 'grad_norm': 18.541139602661133, 'learning_rate': 1.3933333333333334e-05, 'epoch': 0.91}\n",
      "{'loss': 0.316, 'grad_norm': 15.676413536071777, 'learning_rate': 1.3866666666666669e-05, 'epoch': 0.92}\n",
      "{'loss': 0.288, 'grad_norm': 4.637294292449951, 'learning_rate': 1.38e-05, 'epoch': 0.93}\n",
      "{'loss': 0.2714, 'grad_norm': 15.00117301940918, 'learning_rate': 1.3733333333333335e-05, 'epoch': 0.94}\n",
      "{'loss': 0.2374, 'grad_norm': 63.612979888916016, 'learning_rate': 1.3666666666666667e-05, 'epoch': 0.95}\n",
      "{'loss': 0.2785, 'grad_norm': 28.745025634765625, 'learning_rate': 1.3600000000000002e-05, 'epoch': 0.96}\n",
      "{'loss': 0.3621, 'grad_norm': 14.289488792419434, 'learning_rate': 1.3533333333333333e-05, 'epoch': 0.97}\n",
      "{'loss': 0.3026, 'grad_norm': 41.70613479614258, 'learning_rate': 1.3466666666666668e-05, 'epoch': 0.98}\n",
      "{'loss': 0.2357, 'grad_norm': 3.7307357788085938, 'learning_rate': 1.3400000000000002e-05, 'epoch': 0.99}\n",
      "{'loss': 0.3173, 'grad_norm': 2.3716847896575928, 'learning_rate': 1.3333333333333333e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f43f17cd0644af5a5714d3ce7e1b792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2692180871963501, 'eval_accuracy': 0.9125, 'eval_f1': 0.9133763201354372, 'eval_runtime': 13.563, 'eval_samples_per_second': 147.46, 'eval_steps_per_second': 9.216, 'epoch': 1.0}\n",
      "{'loss': 0.2365, 'grad_norm': 49.20100021362305, 'learning_rate': 1.3266666666666668e-05, 'epoch': 1.01}\n",
      "{'loss': 0.2225, 'grad_norm': 4.356011867523193, 'learning_rate': 1.3200000000000002e-05, 'epoch': 1.02}\n",
      "{'loss': 0.1519, 'grad_norm': 27.05234146118164, 'learning_rate': 1.3133333333333334e-05, 'epoch': 1.03}\n",
      "{'loss': 0.1193, 'grad_norm': 12.991595268249512, 'learning_rate': 1.3066666666666668e-05, 'epoch': 1.04}\n",
      "{'loss': 0.2315, 'grad_norm': 13.576393127441406, 'learning_rate': 1.3000000000000001e-05, 'epoch': 1.05}\n",
      "{'loss': 0.2829, 'grad_norm': 15.33315658569336, 'learning_rate': 1.2933333333333334e-05, 'epoch': 1.06}\n",
      "{'loss': 0.1614, 'grad_norm': 6.408905029296875, 'learning_rate': 1.2866666666666667e-05, 'epoch': 1.07}\n",
      "{'loss': 0.2038, 'grad_norm': 11.83803653717041, 'learning_rate': 1.2800000000000001e-05, 'epoch': 1.08}\n",
      "{'loss': 0.0998, 'grad_norm': 8.566740989685059, 'learning_rate': 1.2733333333333336e-05, 'epoch': 1.09}\n",
      "{'loss': 0.2373, 'grad_norm': 2.784853219985962, 'learning_rate': 1.2666666666666667e-05, 'epoch': 1.1}\n",
      "{'loss': 0.1912, 'grad_norm': 39.47752380371094, 'learning_rate': 1.2600000000000001e-05, 'epoch': 1.11}\n",
      "{'loss': 0.2974, 'grad_norm': 12.608796119689941, 'learning_rate': 1.2533333333333336e-05, 'epoch': 1.12}\n",
      "{'loss': 0.2605, 'grad_norm': 22.201377868652344, 'learning_rate': 1.2466666666666667e-05, 'epoch': 1.13}\n",
      "{'loss': 0.2235, 'grad_norm': 64.55242919921875, 'learning_rate': 1.2400000000000002e-05, 'epoch': 1.14}\n",
      "{'loss': 0.2254, 'grad_norm': 24.33651351928711, 'learning_rate': 1.2333333333333334e-05, 'epoch': 1.15}\n",
      "{'loss': 0.2775, 'grad_norm': 1.9703119993209839, 'learning_rate': 1.2266666666666667e-05, 'epoch': 1.16}\n",
      "{'loss': 0.2344, 'grad_norm': 9.935669898986816, 'learning_rate': 1.22e-05, 'epoch': 1.17}\n",
      "{'loss': 0.2246, 'grad_norm': 0.4784517288208008, 'learning_rate': 1.2133333333333335e-05, 'epoch': 1.18}\n",
      "{'loss': 0.2075, 'grad_norm': 9.195269584655762, 'learning_rate': 1.206666666666667e-05, 'epoch': 1.19}\n",
      "{'loss': 0.2004, 'grad_norm': 24.671789169311523, 'learning_rate': 1.2e-05, 'epoch': 1.2}\n",
      "{'loss': 0.1478, 'grad_norm': 10.217205047607422, 'learning_rate': 1.1933333333333335e-05, 'epoch': 1.21}\n",
      "{'loss': 0.2491, 'grad_norm': 18.059415817260742, 'learning_rate': 1.186666666666667e-05, 'epoch': 1.22}\n",
      "{'loss': 0.2265, 'grad_norm': 15.96835994720459, 'learning_rate': 1.18e-05, 'epoch': 1.23}\n",
      "{'loss': 0.2414, 'grad_norm': 15.507291793823242, 'learning_rate': 1.1733333333333335e-05, 'epoch': 1.24}\n",
      "{'loss': 0.1245, 'grad_norm': 27.554075241088867, 'learning_rate': 1.1666666666666668e-05, 'epoch': 1.25}\n",
      "{'loss': 0.2721, 'grad_norm': 44.276363372802734, 'learning_rate': 1.16e-05, 'epoch': 1.26}\n",
      "{'loss': 0.1887, 'grad_norm': 17.484296798706055, 'learning_rate': 1.1533333333333334e-05, 'epoch': 1.27}\n",
      "{'loss': 0.1946, 'grad_norm': 40.48625183105469, 'learning_rate': 1.1466666666666668e-05, 'epoch': 1.28}\n",
      "{'loss': 0.2014, 'grad_norm': 28.47869300842285, 'learning_rate': 1.14e-05, 'epoch': 1.29}\n",
      "{'loss': 0.2064, 'grad_norm': 4.1380295753479, 'learning_rate': 1.1333333333333334e-05, 'epoch': 1.3}\n",
      "{'loss': 0.2066, 'grad_norm': 9.90374755859375, 'learning_rate': 1.1266666666666668e-05, 'epoch': 1.31}\n",
      "{'loss': 0.1768, 'grad_norm': 18.054889678955078, 'learning_rate': 1.1200000000000001e-05, 'epoch': 1.32}\n",
      "{'loss': 0.2009, 'grad_norm': 0.266623854637146, 'learning_rate': 1.1133333333333334e-05, 'epoch': 1.33}\n",
      "{'loss': 0.2021, 'grad_norm': 19.071678161621094, 'learning_rate': 1.1066666666666669e-05, 'epoch': 1.34}\n",
      "{'loss': 0.2269, 'grad_norm': 9.416799545288086, 'learning_rate': 1.1000000000000001e-05, 'epoch': 1.35}\n",
      "{'loss': 0.2517, 'grad_norm': 12.585530281066895, 'learning_rate': 1.0933333333333334e-05, 'epoch': 1.36}\n",
      "{'loss': 0.2518, 'grad_norm': 7.6335344314575195, 'learning_rate': 1.0866666666666667e-05, 'epoch': 1.37}\n",
      "{'loss': 0.1944, 'grad_norm': 19.595796585083008, 'learning_rate': 1.0800000000000002e-05, 'epoch': 1.38}\n",
      "{'loss': 0.1432, 'grad_norm': 14.25472354888916, 'learning_rate': 1.0733333333333333e-05, 'epoch': 1.39}\n",
      "{'loss': 0.26, 'grad_norm': 10.661713600158691, 'learning_rate': 1.0666666666666667e-05, 'epoch': 1.4}\n",
      "{'loss': 0.2257, 'grad_norm': 6.086898326873779, 'learning_rate': 1.0600000000000002e-05, 'epoch': 1.41}\n",
      "{'loss': 0.1524, 'grad_norm': 20.858259201049805, 'learning_rate': 1.0533333333333333e-05, 'epoch': 1.42}\n",
      "{'loss': 0.1423, 'grad_norm': 10.11747932434082, 'learning_rate': 1.0466666666666668e-05, 'epoch': 1.43}\n",
      "{'loss': 0.212, 'grad_norm': 24.522809982299805, 'learning_rate': 1.04e-05, 'epoch': 1.44}\n",
      "{'loss': 0.1706, 'grad_norm': 28.35752296447754, 'learning_rate': 1.0333333333333335e-05, 'epoch': 1.45}\n",
      "{'loss': 0.1605, 'grad_norm': 5.329407691955566, 'learning_rate': 1.0266666666666668e-05, 'epoch': 1.46}\n",
      "{'loss': 0.1853, 'grad_norm': 1.4012329578399658, 'learning_rate': 1.02e-05, 'epoch': 1.47}\n",
      "{'loss': 0.2627, 'grad_norm': 99.67376708984375, 'learning_rate': 1.0133333333333335e-05, 'epoch': 1.48}\n",
      "{'loss': 0.2297, 'grad_norm': 21.295185089111328, 'learning_rate': 1.0066666666666666e-05, 'epoch': 1.49}\n",
      "{'loss': 0.2238, 'grad_norm': 11.510335922241211, 'learning_rate': 1e-05, 'epoch': 1.5}\n",
      "{'loss': 0.2053, 'grad_norm': 58.44978713989258, 'learning_rate': 9.933333333333334e-06, 'epoch': 1.51}\n",
      "{'loss': 0.1703, 'grad_norm': 32.95111846923828, 'learning_rate': 9.866666666666668e-06, 'epoch': 1.52}\n",
      "{'loss': 0.1427, 'grad_norm': 8.353930473327637, 'learning_rate': 9.800000000000001e-06, 'epoch': 1.53}\n",
      "{'loss': 0.1599, 'grad_norm': 0.31066882610321045, 'learning_rate': 9.733333333333334e-06, 'epoch': 1.54}\n",
      "{'loss': 0.0926, 'grad_norm': 0.3500574231147766, 'learning_rate': 9.666666666666667e-06, 'epoch': 1.55}\n",
      "{'loss': 0.2384, 'grad_norm': 13.613800048828125, 'learning_rate': 9.600000000000001e-06, 'epoch': 1.56}\n",
      "{'loss': 0.1586, 'grad_norm': 7.4991655349731445, 'learning_rate': 9.533333333333334e-06, 'epoch': 1.57}\n",
      "{'loss': 0.093, 'grad_norm': 0.29976797103881836, 'learning_rate': 9.466666666666667e-06, 'epoch': 1.58}\n",
      "{'loss': 0.1567, 'grad_norm': 8.889967918395996, 'learning_rate': 9.4e-06, 'epoch': 1.59}\n",
      "{'loss': 0.3022, 'grad_norm': 39.99482727050781, 'learning_rate': 9.333333333333334e-06, 'epoch': 1.6}\n",
      "{'loss': 0.2134, 'grad_norm': 79.57490539550781, 'learning_rate': 9.266666666666667e-06, 'epoch': 1.61}\n",
      "{'loss': 0.2419, 'grad_norm': 0.8146677017211914, 'learning_rate': 9.200000000000002e-06, 'epoch': 1.62}\n",
      "{'loss': 0.2227, 'grad_norm': 3.304776191711426, 'learning_rate': 9.133333333333335e-06, 'epoch': 1.63}\n",
      "{'loss': 0.1365, 'grad_norm': 121.27330017089844, 'learning_rate': 9.066666666666667e-06, 'epoch': 1.64}\n",
      "{'loss': 0.2073, 'grad_norm': 10.141534805297852, 'learning_rate': 9e-06, 'epoch': 1.65}\n",
      "{'loss': 0.2134, 'grad_norm': 1.8053607940673828, 'learning_rate': 8.933333333333333e-06, 'epoch': 1.66}\n",
      "{'loss': 0.1597, 'grad_norm': 0.5162680149078369, 'learning_rate': 8.866666666666668e-06, 'epoch': 1.67}\n",
      "{'loss': 0.2083, 'grad_norm': 11.931412696838379, 'learning_rate': 8.8e-06, 'epoch': 1.68}\n",
      "{'loss': 0.2952, 'grad_norm': 12.854079246520996, 'learning_rate': 8.733333333333333e-06, 'epoch': 1.69}\n",
      "{'loss': 0.2238, 'grad_norm': 100.67877197265625, 'learning_rate': 8.666666666666668e-06, 'epoch': 1.7}\n",
      "{'loss': 0.1831, 'grad_norm': 6.004271030426025, 'learning_rate': 8.6e-06, 'epoch': 1.71}\n",
      "{'loss': 0.2172, 'grad_norm': 2.275376796722412, 'learning_rate': 8.533333333333335e-06, 'epoch': 1.72}\n",
      "{'loss': 0.3102, 'grad_norm': 12.646208763122559, 'learning_rate': 8.466666666666668e-06, 'epoch': 1.73}\n",
      "{'loss': 0.1145, 'grad_norm': 0.27378275990486145, 'learning_rate': 8.400000000000001e-06, 'epoch': 1.74}\n",
      "{'loss': 0.1958, 'grad_norm': 1.3884042501449585, 'learning_rate': 8.333333333333334e-06, 'epoch': 1.75}\n",
      "{'loss': 0.1739, 'grad_norm': 4.909579753875732, 'learning_rate': 8.266666666666667e-06, 'epoch': 1.76}\n",
      "{'loss': 0.1655, 'grad_norm': 11.300389289855957, 'learning_rate': 8.2e-06, 'epoch': 1.77}\n",
      "{'loss': 0.3008, 'grad_norm': 10.137081146240234, 'learning_rate': 8.133333333333334e-06, 'epoch': 1.78}\n",
      "{'loss': 0.1229, 'grad_norm': 11.92052173614502, 'learning_rate': 8.066666666666667e-06, 'epoch': 1.79}\n",
      "{'loss': 0.2189, 'grad_norm': 37.321372985839844, 'learning_rate': 8.000000000000001e-06, 'epoch': 1.8}\n",
      "{'loss': 0.1101, 'grad_norm': 4.2860565185546875, 'learning_rate': 7.933333333333334e-06, 'epoch': 1.81}\n",
      "{'loss': 0.2642, 'grad_norm': 31.658626556396484, 'learning_rate': 7.866666666666667e-06, 'epoch': 1.82}\n",
      "{'loss': 0.1885, 'grad_norm': 7.799375534057617, 'learning_rate': 7.800000000000002e-06, 'epoch': 1.83}\n",
      "{'loss': 0.1923, 'grad_norm': 9.709375381469727, 'learning_rate': 7.733333333333334e-06, 'epoch': 1.84}\n",
      "{'loss': 0.2436, 'grad_norm': 20.535181045532227, 'learning_rate': 7.666666666666667e-06, 'epoch': 1.85}\n",
      "{'loss': 0.2876, 'grad_norm': 17.768489837646484, 'learning_rate': 7.600000000000001e-06, 'epoch': 1.86}\n",
      "{'loss': 0.2566, 'grad_norm': 210.00274658203125, 'learning_rate': 7.533333333333334e-06, 'epoch': 1.87}\n",
      "{'loss': 0.1179, 'grad_norm': 10.02460765838623, 'learning_rate': 7.4666666666666675e-06, 'epoch': 1.88}\n",
      "{'loss': 0.1832, 'grad_norm': 6.7839131355285645, 'learning_rate': 7.4e-06, 'epoch': 1.89}\n",
      "{'loss': 0.1701, 'grad_norm': 62.6983642578125, 'learning_rate': 7.333333333333333e-06, 'epoch': 1.9}\n",
      "{'loss': 0.2146, 'grad_norm': 10.68224048614502, 'learning_rate': 7.266666666666668e-06, 'epoch': 1.91}\n",
      "{'loss': 0.1527, 'grad_norm': 9.904144287109375, 'learning_rate': 7.2000000000000005e-06, 'epoch': 1.92}\n",
      "{'loss': 0.115, 'grad_norm': 7.3349199295043945, 'learning_rate': 7.133333333333334e-06, 'epoch': 1.93}\n",
      "{'loss': 0.1249, 'grad_norm': 16.255447387695312, 'learning_rate': 7.066666666666667e-06, 'epoch': 1.94}\n",
      "{'loss': 0.1709, 'grad_norm': 6.68548059463501, 'learning_rate': 7e-06, 'epoch': 1.95}\n",
      "{'loss': 0.1777, 'grad_norm': 49.079227447509766, 'learning_rate': 6.9333333333333344e-06, 'epoch': 1.96}\n",
      "{'loss': 0.241, 'grad_norm': 23.71317481994629, 'learning_rate': 6.866666666666667e-06, 'epoch': 1.97}\n",
      "{'loss': 0.2303, 'grad_norm': 6.513842582702637, 'learning_rate': 6.800000000000001e-06, 'epoch': 1.98}\n",
      "{'loss': 0.1292, 'grad_norm': 4.062630653381348, 'learning_rate': 6.733333333333334e-06, 'epoch': 1.99}\n",
      "{'loss': 0.1004, 'grad_norm': 3.722520351409912, 'learning_rate': 6.666666666666667e-06, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a622d2b3dc4690a38e2a688d1d88c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.19843173027038574, 'eval_accuracy': 0.929, 'eval_f1': 0.9286921870177579, 'eval_runtime': 13.4479, 'eval_samples_per_second': 148.722, 'eval_steps_per_second': 9.295, 'epoch': 2.0}\n",
      "{'loss': 0.2572, 'grad_norm': 37.28451919555664, 'learning_rate': 6.600000000000001e-06, 'epoch': 2.01}\n",
      "{'loss': 0.1008, 'grad_norm': 6.705952167510986, 'learning_rate': 6.533333333333334e-06, 'epoch': 2.02}\n",
      "{'loss': 0.2049, 'grad_norm': 1.7621207237243652, 'learning_rate': 6.466666666666667e-06, 'epoch': 2.03}\n",
      "{'loss': 0.1511, 'grad_norm': 8.033997535705566, 'learning_rate': 6.4000000000000006e-06, 'epoch': 2.04}\n",
      "{'loss': 0.1481, 'grad_norm': 40.760379791259766, 'learning_rate': 6.333333333333333e-06, 'epoch': 2.05}\n",
      "{'loss': 0.0856, 'grad_norm': 5.293247222900391, 'learning_rate': 6.266666666666668e-06, 'epoch': 2.06}\n",
      "{'loss': 0.2024, 'grad_norm': 30.550016403198242, 'learning_rate': 6.200000000000001e-06, 'epoch': 2.07}\n",
      "{'loss': 0.1551, 'grad_norm': 0.1978943943977356, 'learning_rate': 6.133333333333334e-06, 'epoch': 2.08}\n",
      "{'loss': 0.1011, 'grad_norm': 59.11030960083008, 'learning_rate': 6.066666666666667e-06, 'epoch': 2.09}\n",
      "{'loss': 0.0997, 'grad_norm': 1.9144477844238281, 'learning_rate': 6e-06, 'epoch': 2.1}\n",
      "{'loss': 0.1247, 'grad_norm': 57.900291442871094, 'learning_rate': 5.933333333333335e-06, 'epoch': 2.11}\n",
      "{'loss': 0.1307, 'grad_norm': 3.3796255588531494, 'learning_rate': 5.8666666666666675e-06, 'epoch': 2.12}\n",
      "{'loss': 0.1591, 'grad_norm': 86.47286224365234, 'learning_rate': 5.8e-06, 'epoch': 2.13}\n",
      "{'loss': 0.207, 'grad_norm': 17.29493522644043, 'learning_rate': 5.733333333333334e-06, 'epoch': 2.14}\n",
      "{'loss': 0.2225, 'grad_norm': 7.537704944610596, 'learning_rate': 5.666666666666667e-06, 'epoch': 2.15}\n",
      "{'loss': 0.1171, 'grad_norm': 24.895904541015625, 'learning_rate': 5.600000000000001e-06, 'epoch': 2.16}\n",
      "{'loss': 0.0913, 'grad_norm': 7.010570526123047, 'learning_rate': 5.533333333333334e-06, 'epoch': 2.17}\n",
      "{'loss': 0.2088, 'grad_norm': 82.60974884033203, 'learning_rate': 5.466666666666667e-06, 'epoch': 2.18}\n",
      "{'loss': 0.082, 'grad_norm': 3.093562602996826, 'learning_rate': 5.400000000000001e-06, 'epoch': 2.19}\n",
      "{'loss': 0.2559, 'grad_norm': 22.565160751342773, 'learning_rate': 5.333333333333334e-06, 'epoch': 2.2}\n",
      "{'loss': 0.0817, 'grad_norm': 0.2966987192630768, 'learning_rate': 5.2666666666666665e-06, 'epoch': 2.21}\n",
      "{'loss': 0.0941, 'grad_norm': 8.712961196899414, 'learning_rate': 5.2e-06, 'epoch': 2.22}\n",
      "{'loss': 0.1506, 'grad_norm': 25.350404739379883, 'learning_rate': 5.133333333333334e-06, 'epoch': 2.23}\n",
      "{'loss': 0.1265, 'grad_norm': 30.811107635498047, 'learning_rate': 5.0666666666666676e-06, 'epoch': 2.24}\n",
      "{'loss': 0.17, 'grad_norm': 3.513812303543091, 'learning_rate': 5e-06, 'epoch': 2.25}\n",
      "{'loss': 0.0957, 'grad_norm': 3.522087812423706, 'learning_rate': 4.933333333333334e-06, 'epoch': 2.26}\n",
      "{'loss': 0.1503, 'grad_norm': 3.939002513885498, 'learning_rate': 4.866666666666667e-06, 'epoch': 2.27}\n",
      "{'loss': 0.2309, 'grad_norm': 7.773962020874023, 'learning_rate': 4.800000000000001e-06, 'epoch': 2.28}\n",
      "{'loss': 0.0847, 'grad_norm': 14.411710739135742, 'learning_rate': 4.7333333333333335e-06, 'epoch': 2.29}\n",
      "{'loss': 0.1362, 'grad_norm': 30.42478370666504, 'learning_rate': 4.666666666666667e-06, 'epoch': 2.3}\n",
      "{'loss': 0.0386, 'grad_norm': 0.9760037660598755, 'learning_rate': 4.600000000000001e-06, 'epoch': 2.31}\n",
      "{'loss': 0.1721, 'grad_norm': 11.377111434936523, 'learning_rate': 4.533333333333334e-06, 'epoch': 2.32}\n",
      "{'loss': 0.1294, 'grad_norm': 9.153220176696777, 'learning_rate': 4.4666666666666665e-06, 'epoch': 2.33}\n",
      "{'loss': 0.109, 'grad_norm': 18.225027084350586, 'learning_rate': 4.4e-06, 'epoch': 2.34}\n",
      "{'loss': 0.1133, 'grad_norm': 24.877708435058594, 'learning_rate': 4.333333333333334e-06, 'epoch': 2.35}\n",
      "{'loss': 0.1305, 'grad_norm': 6.650913238525391, 'learning_rate': 4.266666666666668e-06, 'epoch': 2.36}\n",
      "{'loss': 0.0849, 'grad_norm': 0.5532817244529724, 'learning_rate': 4.2000000000000004e-06, 'epoch': 2.37}\n",
      "{'loss': 0.1233, 'grad_norm': 8.345237731933594, 'learning_rate': 4.133333333333333e-06, 'epoch': 2.38}\n",
      "{'loss': 0.2415, 'grad_norm': 3.2255918979644775, 'learning_rate': 4.066666666666667e-06, 'epoch': 2.39}\n",
      "{'loss': 0.0574, 'grad_norm': 1.7304997444152832, 'learning_rate': 4.000000000000001e-06, 'epoch': 2.4}\n",
      "{'loss': 0.1168, 'grad_norm': 13.216453552246094, 'learning_rate': 3.9333333333333335e-06, 'epoch': 2.41}\n",
      "{'loss': 0.1216, 'grad_norm': 5.933935642242432, 'learning_rate': 3.866666666666667e-06, 'epoch': 2.42}\n",
      "{'loss': 0.114, 'grad_norm': 14.900193214416504, 'learning_rate': 3.8000000000000005e-06, 'epoch': 2.43}\n",
      "{'loss': 0.2311, 'grad_norm': 10.832079887390137, 'learning_rate': 3.7333333333333337e-06, 'epoch': 2.44}\n",
      "{'loss': 0.1503, 'grad_norm': 15.051475524902344, 'learning_rate': 3.6666666666666666e-06, 'epoch': 2.45}\n",
      "{'loss': 0.1783, 'grad_norm': 78.48104095458984, 'learning_rate': 3.6000000000000003e-06, 'epoch': 2.46}\n",
      "{'loss': 0.067, 'grad_norm': 0.7780535221099854, 'learning_rate': 3.5333333333333335e-06, 'epoch': 2.47}\n",
      "{'loss': 0.1121, 'grad_norm': 0.9728487133979797, 'learning_rate': 3.4666666666666672e-06, 'epoch': 2.48}\n",
      "{'loss': 0.0881, 'grad_norm': 0.06422485411167145, 'learning_rate': 3.4000000000000005e-06, 'epoch': 2.49}\n",
      "{'loss': 0.1464, 'grad_norm': 0.05889137089252472, 'learning_rate': 3.3333333333333333e-06, 'epoch': 2.5}\n",
      "{'loss': 0.1668, 'grad_norm': 10.02458667755127, 'learning_rate': 3.266666666666667e-06, 'epoch': 2.51}\n",
      "{'loss': 0.201, 'grad_norm': 14.83562183380127, 'learning_rate': 3.2000000000000003e-06, 'epoch': 2.52}\n",
      "{'loss': 0.1034, 'grad_norm': 32.128360748291016, 'learning_rate': 3.133333333333334e-06, 'epoch': 2.53}\n",
      "{'loss': 0.0751, 'grad_norm': 1.1839079856872559, 'learning_rate': 3.066666666666667e-06, 'epoch': 2.54}\n",
      "{'loss': 0.0993, 'grad_norm': 6.654170989990234, 'learning_rate': 3e-06, 'epoch': 2.55}\n",
      "{'loss': 0.0609, 'grad_norm': 21.971153259277344, 'learning_rate': 2.9333333333333338e-06, 'epoch': 2.56}\n",
      "{'loss': 0.1067, 'grad_norm': 36.49800109863281, 'learning_rate': 2.866666666666667e-06, 'epoch': 2.57}\n",
      "{'loss': 0.054, 'grad_norm': 83.52749633789062, 'learning_rate': 2.8000000000000003e-06, 'epoch': 2.58}\n",
      "{'loss': 0.1854, 'grad_norm': 26.768516540527344, 'learning_rate': 2.7333333333333336e-06, 'epoch': 2.59}\n",
      "{'loss': 0.187, 'grad_norm': 0.49431559443473816, 'learning_rate': 2.666666666666667e-06, 'epoch': 2.6}\n",
      "{'loss': 0.1, 'grad_norm': 0.05395079404115677, 'learning_rate': 2.6e-06, 'epoch': 2.61}\n",
      "{'loss': 0.0717, 'grad_norm': 27.641672134399414, 'learning_rate': 2.5333333333333338e-06, 'epoch': 2.62}\n",
      "{'loss': 0.1001, 'grad_norm': 0.06166623905301094, 'learning_rate': 2.466666666666667e-06, 'epoch': 2.63}\n",
      "{'loss': 0.1992, 'grad_norm': 3.6661529541015625, 'learning_rate': 2.4000000000000003e-06, 'epoch': 2.64}\n",
      "{'loss': 0.1398, 'grad_norm': 14.27582836151123, 'learning_rate': 2.3333333333333336e-06, 'epoch': 2.65}\n",
      "{'loss': 0.1353, 'grad_norm': 13.438613891601562, 'learning_rate': 2.266666666666667e-06, 'epoch': 2.66}\n",
      "{'loss': 0.1977, 'grad_norm': 5.401202201843262, 'learning_rate': 2.2e-06, 'epoch': 2.67}\n",
      "{'loss': 0.1034, 'grad_norm': 14.248496055603027, 'learning_rate': 2.133333333333334e-06, 'epoch': 2.68}\n",
      "{'loss': 0.2116, 'grad_norm': 8.226641654968262, 'learning_rate': 2.0666666666666666e-06, 'epoch': 2.69}\n",
      "{'loss': 0.137, 'grad_norm': 10.824552536010742, 'learning_rate': 2.0000000000000003e-06, 'epoch': 2.7}\n",
      "{'loss': 0.1779, 'grad_norm': 42.349822998046875, 'learning_rate': 1.9333333333333336e-06, 'epoch': 2.71}\n",
      "{'loss': 0.1098, 'grad_norm': 14.97117805480957, 'learning_rate': 1.8666666666666669e-06, 'epoch': 2.72}\n",
      "{'loss': 0.0876, 'grad_norm': 10.613402366638184, 'learning_rate': 1.8000000000000001e-06, 'epoch': 2.73}\n",
      "{'loss': 0.1561, 'grad_norm': 6.274112224578857, 'learning_rate': 1.7333333333333336e-06, 'epoch': 2.74}\n",
      "{'loss': 0.1253, 'grad_norm': 8.156272888183594, 'learning_rate': 1.6666666666666667e-06, 'epoch': 2.75}\n",
      "{'loss': 0.0499, 'grad_norm': 5.717106342315674, 'learning_rate': 1.6000000000000001e-06, 'epoch': 2.76}\n",
      "{'loss': 0.1493, 'grad_norm': 2.8428564071655273, 'learning_rate': 1.5333333333333334e-06, 'epoch': 2.77}\n",
      "{'loss': 0.1113, 'grad_norm': 11.138758659362793, 'learning_rate': 1.4666666666666669e-06, 'epoch': 2.78}\n",
      "{'loss': 0.0657, 'grad_norm': 0.04557245224714279, 'learning_rate': 1.4000000000000001e-06, 'epoch': 2.79}\n",
      "{'loss': 0.1416, 'grad_norm': 20.039270401000977, 'learning_rate': 1.3333333333333334e-06, 'epoch': 2.8}\n",
      "{'loss': 0.1021, 'grad_norm': 5.118952751159668, 'learning_rate': 1.2666666666666669e-06, 'epoch': 2.81}\n",
      "{'loss': 0.1761, 'grad_norm': 0.9116824269294739, 'learning_rate': 1.2000000000000002e-06, 'epoch': 2.82}\n",
      "{'loss': 0.1598, 'grad_norm': 152.11460876464844, 'learning_rate': 1.1333333333333334e-06, 'epoch': 2.83}\n",
      "{'loss': 0.0927, 'grad_norm': 6.080542087554932, 'learning_rate': 1.066666666666667e-06, 'epoch': 2.84}\n",
      "{'loss': 0.094, 'grad_norm': 2.265359401702881, 'learning_rate': 1.0000000000000002e-06, 'epoch': 2.85}\n",
      "{'loss': 0.1319, 'grad_norm': 3.026822090148926, 'learning_rate': 9.333333333333334e-07, 'epoch': 2.86}\n",
      "{'loss': 0.1314, 'grad_norm': 10.577908515930176, 'learning_rate': 8.666666666666668e-07, 'epoch': 2.87}\n",
      "{'loss': 0.1358, 'grad_norm': 26.054771423339844, 'learning_rate': 8.000000000000001e-07, 'epoch': 2.88}\n",
      "{'loss': 0.1121, 'grad_norm': 15.383073806762695, 'learning_rate': 7.333333333333334e-07, 'epoch': 2.89}\n",
      "{'loss': 0.1237, 'grad_norm': 6.689915657043457, 'learning_rate': 6.666666666666667e-07, 'epoch': 2.9}\n",
      "{'loss': 0.1346, 'grad_norm': 5.3837995529174805, 'learning_rate': 6.000000000000001e-07, 'epoch': 2.91}\n",
      "{'loss': 0.0928, 'grad_norm': 125.8276138305664, 'learning_rate': 5.333333333333335e-07, 'epoch': 2.92}\n",
      "{'loss': 0.0853, 'grad_norm': 21.86941146850586, 'learning_rate': 4.666666666666667e-07, 'epoch': 2.93}\n",
      "{'loss': 0.1462, 'grad_norm': 30.686115264892578, 'learning_rate': 4.0000000000000003e-07, 'epoch': 2.94}\n",
      "{'loss': 0.0899, 'grad_norm': 7.3888773918151855, 'learning_rate': 3.3333333333333335e-07, 'epoch': 2.95}\n",
      "{'loss': 0.0936, 'grad_norm': 9.800713539123535, 'learning_rate': 2.666666666666667e-07, 'epoch': 2.96}\n",
      "{'loss': 0.1071, 'grad_norm': 5.494946479797363, 'learning_rate': 2.0000000000000002e-07, 'epoch': 2.97}\n",
      "{'loss': 0.0737, 'grad_norm': 7.500021457672119, 'learning_rate': 1.3333333333333336e-07, 'epoch': 2.98}\n",
      "{'loss': 0.1299, 'grad_norm': 4.513332843780518, 'learning_rate': 6.666666666666668e-08, 'epoch': 2.99}\n",
      "{'loss': 0.1697, 'grad_norm': 2.50545072555542, 'learning_rate': 0.0, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce58b2d078ac4d9aa5333f623d4ccd47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.16054922342300415, 'eval_accuracy': 0.9385, 'eval_f1': 0.938676513529829, 'eval_runtime': 13.5911, 'eval_samples_per_second': 147.155, 'eval_steps_per_second': 9.197, 'epoch': 3.0}\n",
      "{'train_runtime': 9093.5713, 'train_samples_per_second': 5.278, 'train_steps_per_second': 0.33, 'train_loss': 0.33017677734295525, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3000, training_loss=0.33017677734295525, metrics={'train_runtime': 9093.5713, 'train_samples_per_second': 5.278, 'train_steps_per_second': 0.33, 'total_flos': 3157446057984000.0, 'train_loss': 0.33017677734295525, 'epoch': 3.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "465b7b84a68843c78d494213ed69ae70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.16807608306407928, 'eval_accuracy': 0.929, 'eval_f1': 0.9285057334865384, 'eval_runtime': 13.6803, 'eval_samples_per_second': 146.195, 'eval_steps_per_second': 9.137, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "184f040f3d73449dad3d02bd4a2ac527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     sadness       0.96      0.97      0.97       581\n",
      "         joy       0.95      0.95      0.95       695\n",
      "        love       0.83      0.81      0.82       159\n",
      "       anger       0.94      0.92      0.93       275\n",
      "        fear       0.86      0.94      0.90       224\n",
      "    surprise       0.80      0.68      0.74        66\n",
      "\n",
      "    accuracy                           0.93      2000\n",
      "   macro avg       0.89      0.88      0.88      2000\n",
      "weighted avg       0.93      0.93      0.93      2000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./emotion_model\\\\tokenizer_config.json',\n",
       " './emotion_model\\\\special_tokens_map.json',\n",
       " './emotion_model\\\\sentencepiece.bpe.model',\n",
       " './emotion_model\\\\added_tokens.json',\n",
       " './emotion_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = trainer.evaluate(test_dataset)\n",
    "print(\"Evaluation Results:\", results)\n",
    "\n",
    "test_logits = trainer.predict(test_dataset).predictions\n",
    "y_pred = np.argmax(test_logits, axis=1)\n",
    "print(classification_report(test_labels, y_pred, target_names=emotion_labels))\n",
    "\n",
    "model.save_pretrained(\"./emotion_model\")\n",
    "tokenizer.save_pretrained(\"./emotion_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(\"./emotion_model\")\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(\"./emotion_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I feel really happy today! => Predicted Emotion: joy\n",
      "Text: I'm so angry right now! => Predicted Emotion: anger\n",
      "Text: This is the worst day ever. => Predicted Emotion: sadness\n"
     ]
    }
   ],
   "source": [
    "def predict_emotion(texts):\n",
    "    encodings = loaded_tokenizer(\n",
    "        texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    outputs = loaded_model(**encodings)\n",
    "    predictions = torch.argmax(outputs.logits, axis=1).numpy()\n",
    "    return [emotion_labels[pred] for pred in predictions]\n",
    "\n",
    "new_texts = [\n",
    "    \"I feel really happy today!\",\n",
    "    \"I'm so angry right now!\",\n",
    "    \"This is the worst day ever.\",\n",
    "]\n",
    "predictions = predict_emotion(new_texts)\n",
    "for text, emotion in zip(new_texts, predictions):\n",
    "    print(f\"Text: {text} => Predicted Emotion: {emotion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1a8248278574f0bb0ae882e764d5ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\anaconda3\\envs\\myenv\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\lenovo\\.cache\\huggingface\\hub\\models--Rajkumar57--englishsentiment-model. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "334ceacdc67942f58fc5180b3649a0b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cc12b108f35404cb700eb3a3f17cd48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5c292e823cc4746826aaa7bc535d07f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/295 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77d023be31e24993ac055accc58bf0b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecdb1fc020c541d490ea0b2a01905572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I feel really happy today! => Predicted Emotion: joy\n",
      "Text: I'm so angry right now! => Predicted Emotion: anger\n",
      "Text: This is the worst day ever. => Predicted Emotion: sadness\n"
     ]
    }
   ],
   "source": [
    "#Hugging face upload model\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "emotion_labels = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Rajkumar57/englishsentiment-model\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"Rajkumar57/englishsentiment-model\")\n",
    "\n",
    "def predict_emotion(texts):\n",
    "    encodings = tokenizer(\n",
    "        texts,\n",
    "        padding=True,       \n",
    "        truncation=True,    \n",
    "        max_length=128,     \n",
    "        return_tensors=\"pt\" \n",
    "    )\n",
    "\n",
    "    outputs = model(**encodings)\n",
    "\n",
    "    predictions = torch.argmax(outputs.logits, axis=1).numpy()\n",
    "\n",
    "    return [emotion_labels[pred] for pred in predictions]\n",
    "\n",
    "new_texts = [\n",
    "    \"I feel really happy today!\",\n",
    "    \"I'm so angry right now!\",\n",
    "    \"This is the worst day ever.\",\n",
    "]\n",
    "\n",
    "predictions = predict_emotion(new_texts)\n",
    "\n",
    "for text, emotion in zip(new_texts, predictions):\n",
    "    print(f\"Text: {text} => Predicted Emotion: {emotion}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
